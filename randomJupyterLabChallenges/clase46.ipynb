{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# Creamos objects a partir de las clases PorterStemmer y LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LucasUser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LucasUser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LucasUser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')    # esta linea deben ejecutarla sólo una vez\n",
    "nltk.download('stopwords') # esta linea deben ejecutarla sólo una vez\n",
    "nltk.download('wordnet')    # esta linea deben ejecutarla sólo una vez\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the first sentence.', 'A gallon of milk in the U.S. costs $2.99.', 'Is this the third sentence?', 'Yes, it is!']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. \\\n",
    "    Is this the third sentence? Yes, it is!\"\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsEN = stopwords.words('english')\n",
    "wordsES = stopwords.words('spanish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "# Creamos una lista de palabras para hacer stemming con ambos algoritmos:\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\", \\\n",
    "             \"stabil\",\"destabilize\",\"misunderstanding\",\\\n",
    "             \"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos textos \n",
    "\n",
    "t0 = \"El potro y el angel llegaron al cine por casualidad.\"\n",
    "t1 = \"El ángel, el tanque del cine nacional, un paso más cerca del oscar\"\n",
    "t2 = \"final del mes del cine nacional: 'El Potro', la única cinta 'millonaria'\"\n",
    "t3 = \"Juan Martin del potro volvió a tandil: se dio el ultimo baño de masas con los suyos.\"\n",
    "t4 = \"Juan Martin del potro fue recibido por una multitud en Tandil.\"\n",
    "t5=  \"Juan Martin del potro fue a ver 'El Potro' al cine y le encantó.\"\n",
    "textos=[t0,t1,t2,t3,t4,t5];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El potro y el angel llegaron al cine por casualidad.',\n",
       " 'El ángel, el tanque del cine nacional, un paso más cerca del oscar',\n",
       " \"final del mes del cine nacional: 'El Potro', la única cinta 'millonaria'\",\n",
       " 'Juan Martin del potro volvió a tandil: se dio el ultimo baño de masas con los suyos.',\n",
       " 'Juan Martin del potro fue recibido por una multitud en Tandil.',\n",
       " \"Juan Martin del potro fue a ver 'El Potro' al cine y le encantó.\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario:\n",
      " {'potro': 19, 'angel': 0, 'llegaron': 10, 'cine': 4, 'casualidad': 2, 'tanque': 23, 'nacional': 16, 'paso': 18, 'cerca': 3, 'oscar': 17, 'final': 8, 'mes': 13, 'unica': 25, 'cinta': 5, 'millonaria': 14, 'juan': 9, 'martin': 11, 'volvio': 27, 'tandil': 22, 'dio': 6, 'ultimo': 24, 'bano': 1, 'masas': 12, 'suyos': 21, 'recibido': 20, 'multitud': 15, 'ver': 26, 'encanto': 7}\n"
     ]
    }
   ],
   "source": [
    "stopwords_sp = stopwords.words('spanish');\n",
    "spanishStemmer=SnowballStemmer(\"spanish\")\n",
    "\n",
    "# si no hacemos esto y usamos directo stopwords_sp, CountVectorizer devuelve un warning\n",
    "stopwords_sp_stem = [spanishStemmer.stem(x) for x in stopwords_sp]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = stopwords_sp_stem, lowercase = True, strip_accents = 'unicode');\n",
    "\n",
    "vectorizer.fit(textos);\n",
    "print('Vocabulario:\\n',vectorizer.vocabulary_) # vocabulario del corpus con la frecuencia de cada término"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Transformamos los textos a una matriz esparsa: <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LucasUser\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>angel</th>\n",
       "      <th>bano</th>\n",
       "      <th>casualidad</th>\n",
       "      <th>cerca</th>\n",
       "      <th>cine</th>\n",
       "      <th>cinta</th>\n",
       "      <th>dio</th>\n",
       "      <th>encanto</th>\n",
       "      <th>final</th>\n",
       "      <th>juan</th>\n",
       "      <th>...</th>\n",
       "      <th>paso</th>\n",
       "      <th>potro</th>\n",
       "      <th>recibido</th>\n",
       "      <th>suyos</th>\n",
       "      <th>tandil</th>\n",
       "      <th>tanque</th>\n",
       "      <th>ultimo</th>\n",
       "      <th>unica</th>\n",
       "      <th>ver</th>\n",
       "      <th>volvio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   angel  bano  casualidad  cerca  cine  cinta  dio  encanto  final  juan  \\\n",
       "0      1     0           1      0     1      0    0        0      0     0   \n",
       "1      1     0           0      1     1      0    0        0      0     0   \n",
       "2      0     0           0      0     1      1    0        0      1     0   \n",
       "3      0     1           0      0     0      0    1        0      0     1   \n",
       "4      0     0           0      0     0      0    0        0      0     1   \n",
       "5      0     0           0      0     1      0    0        1      0     1   \n",
       "\n",
       "   ...  paso  potro  recibido  suyos  tandil  tanque  ultimo  unica  ver  \\\n",
       "0  ...     0      1         0      0       0       0       0      0    0   \n",
       "1  ...     1      0         0      0       0       1       0      0    0   \n",
       "2  ...     0      1         0      0       0       0       0      1    0   \n",
       "3  ...     0      1         0      1       1       0       1      0    0   \n",
       "4  ...     0      1         1      0       1       0       0      0    0   \n",
       "5  ...     0      2         0      0       0       0       0      0    1   \n",
       "\n",
       "   volvio  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       1  \n",
       "4       0  \n",
       "5       0  \n",
       "\n",
       "[6 rows x 28 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvectorizer_encoding = vectorizer.transform(textos);\n",
    "print('\\n Transformamos los textos a una matriz esparsa:',type(countvectorizer_encoding))\n",
    "\n",
    "pd.DataFrame(countvectorizer_encoding.todense(), \n",
    "             columns = vectorizer.get_feature_names()) # Usamos el método .todense() para ver la matriz completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has \\\n",
    "bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "sentence_words = word_tokenize(sentence);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'was',\n",
       " 'running',\n",
       " 'and',\n",
       " 'eating',\n",
       " 'at',\n",
       " 'same',\n",
       " 'time',\n",
       " '.',\n",
       " 'He',\n",
       " 'has',\n",
       " 'bad',\n",
       " 'habit',\n",
       " 'of',\n",
       " 'swimming',\n",
       " 'after',\n",
       " 'playing',\n",
       " 'long',\n",
       " 'hours',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Sun',\n",
       " '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_words"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0fa1436fe2bfd670cc26339aa4c8ea5e427934545de93852db63c2faf7e982b2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
